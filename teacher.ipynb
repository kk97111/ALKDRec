{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee02346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AttMix_T'#['FPMC_T','AttMix_T','STAMP_T']\n",
    "data = 'Games' #['Games','ML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e4effa-adad-4b56-a4f5-2a0558bf5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from utils.dataset import Dataset,DataIterator,get_DataLoader\n",
    "\n",
    "def parse_args(name,model_name):   \n",
    "    parser = argparse.ArgumentParser(description=\"Run .\")  \n",
    "    parser.add_argument('--model', nargs='?', default=model_name)\n",
    "    parser.add_argument('--dataset', nargs='?', default=name,\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                        help='Batch size.')\n",
    "    parser.add_argument('--hidden_factor', type=int, default=100,\n",
    "                        help='Number of hidden factors.')\n",
    "    parser.add_argument('--lamda', type=float, default = 10e-5,\n",
    "                        help='Regularizer for bilinear part.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--per_test', type=int, default=20,\n",
    "                        help='Learning rate.')   \n",
    "    parser.add_argument('--topN', type=int, default=50,\n",
    "                        help='Learning rate.')  \n",
    "    \n",
    "    \n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55f6f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data: meta_data\n",
      "\n",
      "loading data: interaction_data\n",
      "\n",
      "split data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(data,model_name)\n",
    "data = Dataset(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f127d1-2acf-4d9a-bc72-6c76942edbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping = data.item2id\n",
    "# label_type =  type([mapping[key] for i,key in enumerate(mapping) if i==0][0])\n",
    "# for col in df.columns:\n",
    "#     # 检查列中的数据类型\n",
    "#     if isinstance(df[col][0], list):\n",
    "#         # 列表类型，遍历列表替换\n",
    "#         df[col] = df[col].apply(lambda lst: [mapping[item] for item in lst if item in mapping])           \n",
    "#     else:\n",
    "#         df[col] = df[col].map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ae8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.log import LOG\n",
    "log = LOG(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02253da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using time span 128\n",
      "total session: 12054\n",
      "Using time span 128\n",
      "total session: 4018\n",
      "Using time span 128\n",
      "total session: 4019\n"
     ]
    }
   ],
   "source": [
    "train_data =  get_DataLoader(data.train,args.batch_size, seq_len=10)\n",
    "valid_data =  get_DataLoader(data.valid,args.batch_size, seq_len=10,train_flag=0)\n",
    "test_data =  get_DataLoader(data.test,args.batch_size, seq_len=10,train_flag=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cef37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AttMix' in model_name:\n",
    "    from AttMix import AttMix\n",
    "    model = AttMix(data.n_item,args.hidden_factor,args.batch_size,args)\n",
    "if 'FPMC' in model_name:\n",
    "    from FPMC import FPMC\n",
    "    model = FPMC(data.n_item,args.hidden_factor,args.batch_size)\n",
    "if 'STAMP' in model_name:\n",
    "    from STAMP import STAMP\n",
    "    model = STAMP(data.n_item,args.hidden_factor,args.batch_size)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb455172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "iter: 0, train loss: 0.6935, valid recall: 0.000747, valid ndcg: 0.000224\n",
      "time interval: 0.0106 min\n",
      "20\n",
      "iter: 20, train loss: 0.6627, valid recall: 0.003235, valid ndcg: 0.000861\n",
      "time interval: 0.0103 min\n",
      "40\n",
      "iter: 40, train loss: 0.6267, valid recall: 0.008711, valid ndcg: 0.002649\n",
      "time interval: 0.0083 min\n",
      "60\n",
      "iter: 60, train loss: 0.5919, valid recall: 0.015928, valid ndcg: 0.004664\n",
      "time interval: 0.0096 min\n",
      "80\n",
      "iter: 80, train loss: 0.5620, valid recall: 0.022648, valid ndcg: 0.006268\n",
      "time interval: 0.0081 min\n",
      "100\n",
      "iter: 100, train loss: 0.5362, valid recall: 0.028621, valid ndcg: 0.007809\n",
      "time interval: 0.0100 min\n",
      "120\n",
      "iter: 120, train loss: 0.5137, valid recall: 0.031608, valid ndcg: 0.008920\n",
      "time interval: 0.0078 min\n",
      "140\n",
      "iter: 140, train loss: 0.4898, valid recall: 0.034843, valid ndcg: 0.010204\n",
      "time interval: 0.0096 min\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.evaluation import evaluate\n",
    "from utils.log import load_model, save_model\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "def to_tensor(var, device):\n",
    "    var = torch.Tensor(var)\n",
    "    var = var.to(device)\n",
    "    return var.long()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)#, weight_decay=args.weight_decay)\n",
    "best_metric = -1\n",
    "for iter, (targets, items, mask,_) in enumerate(train_data):\n",
    "    #训练\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    targets_cuda = to_tensor(targets,'cuda')\n",
    "    items_cuda = to_tensor(items,'cuda')\n",
    "    mask_cuda = to_tensor(mask,'cuda')\n",
    "    negative_cuda = to_tensor(data.uniform_negative_sample(targets_cuda,10),'cuda')\n",
    "\n",
    "    user_eb, scores = model(items_cuda,mask_cuda)\n",
    "    loss = model.loss(user_eb,targets_cuda,negative_cuda)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % args.per_test  == 0:#\n",
    "        start_time = time.time()\n",
    "        print(iter)\n",
    "        model.eval()\n",
    "        metrics = evaluate(model, valid_data,25,args=args)\n",
    "        log_str = 'iter: %d, train loss: %.4f' % (iter, loss) # 打印loss\n",
    "        if metrics != {}:\n",
    "            log_str += ', ' + ', '.join(['valid ' + key + ': %.6f' % value for key, value in metrics.items()])\n",
    "        print(log_str)\n",
    "        log.write_str(log_str)\n",
    "        # 保存recall最佳的模型\n",
    "        if 'recall' in metrics:\n",
    "            recall = metrics['recall']\n",
    "            if recall > best_metric:\n",
    "                best_metric = recall\n",
    "                save_model(model, log.best_model_path)\n",
    "                trials = 0\n",
    "            else:\n",
    "                trials += 1\n",
    "                args.patience = 20 #if args.dataset =='rocket' else 3 \n",
    "                if trials > args.patience: # early stopping\n",
    "                    print(\"early stopping!\")\n",
    "                    break\n",
    "        # 每次test之后loss_sum置零\n",
    "        total_loss = 0.0\n",
    "        test_time = time.time()\n",
    "        print(\"time interval: %.4f min\" % ((test_time-start_time)/60.0))\n",
    "        sys.stdout.flush()\n",
    "    if iter >=  10000: # 超过最大迭代次数，退出训练\n",
    "        break\n",
    "\n",
    "load_model(model, log.best_model_path)\n",
    "model.eval()\n",
    "\n",
    "# 训练结束后用valid_data测试一次\n",
    "metrics = evaluate(model, valid_data,50,args=args)\n",
    "print(', '.join(['Valid ' + key + ': %.6f' % value for key, value in metrics.items()]))\n",
    "# 训练结束后用test_data测试一次\n",
    "print(\"Test result:\")\n",
    "metrics = evaluate(model, test_data,5,args=args)\n",
    "for key, value in metrics.items():\n",
    "    output = 'test ' + key + '@5' + '=%.6f' % value\n",
    "    print(output)\n",
    "    log.write_str(output)\n",
    "metrics = evaluate(model, test_data,10,args=args)\n",
    "for key, value in metrics.items():\n",
    "    output = 'test ' + key + '@10' + '=%.6f' % value\n",
    "    print(output)\n",
    "    log.write_str(output)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84a08d-81fa-4439-8c0e-ce359b391584",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05ee94-d5fb-4c70-8ffe-53c53b2e8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680aa72c-3c5a-492b-af19-70b1302a1cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyp",
   "language": "python",
   "name": "dyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
