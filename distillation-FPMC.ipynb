{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee02346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation = 'ALKERec'#['ALKERec','RAD-BC','random','easiest','hardest','TR']\n",
    "name = 'Games'#['Games','ML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a73cf4-8313-48cb-855b-d1c34c66082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "from utils.dataset import Dataset,DataIterator,get_DataLoader\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "def parse_args(name):   \n",
    "    parser = argparse.ArgumentParser(description=\"Run .\")  \n",
    "    parser.add_argument('--model', nargs='?', default='PFMC_S_distill')\n",
    "    parser.add_argument('--dataset', nargs='?', default=name,\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                        help='Batch size.')\n",
    "    parser.add_argument('--hidden_factor', type=int, default=10,\n",
    "                        help='Number of hidden factors.')\n",
    "    parser.add_argument('--lamda', type=float, default = 10e-5,\n",
    "                        help='Regularizer for bilinear part.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--per_test', type=int, default=20,\n",
    "                        help='Learning rate.')   \n",
    "    parser.add_argument('--topN', type=int, default=50,\n",
    "                        help='Learning rate.')  \n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23339538-91a8-4984-8299-ea4e6d2f7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def demonstrate_randomness(seed=2024):\n",
    "    # 设置 Python 内置的随机种子\n",
    "    random.seed(seed)    \n",
    "    # 设置 Numpy 的随机种子\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 设置 PyTorch 的随机种子\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 确保 PyTorch 的一致性行为\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 调用函数\n",
    "demonstrate_randomness()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55f6f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data: meta_data\n",
      "\n",
      "loading data: interaction_data\n",
      "\n",
      "split data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(name)\n",
    "data = Dataset(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ae8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.log import LOG, load_model, save_model\n",
    "log = LOG(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02253da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using time span 128\n",
      "total session: 7393\n",
      "Using time span 128\n",
      "total session: 2465\n",
      "Using time span 128\n",
      "total session: 2465\n"
     ]
    }
   ],
   "source": [
    "train_data =  get_DataLoader(data.train,args.batch_size, seq_len=10)\n",
    "valid_data =  get_DataLoader(data.valid,args.batch_size, seq_len=10,train_flag=0)\n",
    "test_data =  get_DataLoader(data.test,args.batch_size, seq_len=10,train_flag=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cef37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FPMC import FPMC\n",
    "model = FPMC(data.n_item,args.hidden_factor,args.batch_size)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4aa2bd-6b56-435d-bcd0-bddf3d6be578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded from ./best_model/ML+FPMC_S\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import calculate_session_embs\n",
    "BASE = FPMC\n",
    "best_model_path_student = './best_model/'+'%s+FPMC_S'%args.dataset\n",
    "#load Student model\n",
    "model = BASE(data.n_item,args.hidden_factor,args.batch_size)    \n",
    "load_model(model, best_model_path_student)\n",
    "model = model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d52ac22-105a-43e4-9e55-2d4e1eaa588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database_LLM(object):\n",
    "    def __init__(self,path):\n",
    "        #|session_id|rec_result|hint_position|\n",
    "        self.dir = path\n",
    "        self.exist_sessions()\n",
    "    def exist_sessions(self):\n",
    "        # 计算文件的行数\n",
    "        self.exist_session = list()\n",
    "        with open(self.dir, 'r', encoding='utf-8') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                session_id, rec_result, hint_position = line.split('\\t')\n",
    "                self.exist_session.append(int(session_id))\n",
    "    def write(self,session_id,rec_result,hint_position):\n",
    "        with open(self.dir,'a') as f:\n",
    "            f.write(\"%d\\t%s\\t%d\\n\"%(session_id,str(rec_result),hint_position))\n",
    "        self.exist_sessions()\n",
    "    def get_all_dict(self):\n",
    "        table_rec = dict()\n",
    "        table_hit = dict()\n",
    "        with open(self.dir, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                session_id,rec_result,hint_position =line.split('\\t')\n",
    "                table_rec[int(session_id)] = eval(rec_result)\n",
    "                table_hit[int(session_id)] = eval(hint_position[:-1])\n",
    "        return table_rec,table_hit\n",
    "    def write_sub_database(self,sub_dir,session_id):\n",
    "        with open(sub_dir,'a') as f:\n",
    "            f.write(\"%d\\n\"%session_id)\n",
    "    def read_sub_database(self,sub_dir,st=0,end=50000):\n",
    "        r = set()\n",
    "        with open(sub_dir) as f:\n",
    "            for i,line in enumerate(f):\n",
    "                if i >= st and i <= end:\n",
    "                    r.add(int(line[:-1]))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2a7c03-58ef-4d2e-a8f0-0ae058a6b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './datasets/%s/%s_LLM_Database.data'%(args.dataset,args.dataset)\n",
    "database = Database_LLM(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b4eb10-9c57-4b2b-b9a1-d525571bd8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('./datasets/%s/%s_prediction.pkl'%(args.dataset,args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81033ca3-7096-4b3b-86b7-d58d417639dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4057542/4249407072.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['LLM_pred'] = [table_rec[session_id] for session_id in sub_df.session_id]\n",
      "/tmp/ipykernel_4057542/4249407072.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['LLM_hit'] = [table_hit[session_id] for session_id in sub_df.session_id]\n"
     ]
    }
   ],
   "source": [
    "sub_df = df[df['session_id'].isin(database.exist_session)]\n",
    "table_rec,table_hit = database.get_all_dict()\n",
    "sub_df['LLM_pred'] = [table_rec[session_id] for session_id in sub_df.session_id]\n",
    "sub_df['LLM_hit'] = [table_hit[session_id] for session_id in sub_df.session_id]\n",
    "print(len(sub_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4ec39b9-a774-4af4-9c0a-44fd841003af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if ablation == 'TR':\n",
    "    select_df = df\n",
    "else:\n",
    "    ablation_sessions = database.read_sub_database('./datasets/%s/%s_%s.base'%(args.dataset,args.dataset,ablation),0,499)\n",
    "    select_df =  sub_df[sub_df['session_id'].isin(ablation_sessions)]\n",
    "    select_df = select_df\n",
    "    other_df = df[~df['session_id'].isin(ablation_sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef431c3b-129a-4cad-bff8-fa551344ab22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(select_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c6c0e5b-e24e-4ca6-89ef-f12fa327b069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.548\n",
      "0.548\n"
     ]
    }
   ],
   "source": [
    "if ablation != 'TR':\n",
    "    #k1 + k2\n",
    "    print(len(select_df[select_df.LLM_hit<25])/len(select_df))\n",
    "    #k2\n",
    "    print(len(select_df[(select_df.difference <= np.median(select_df.difference.values)) & (select_df.LLM_hit<25)])/len(select_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fc73fca-dea1-45d8-8ae8-29b81b79ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = []\n",
    "negative_sampler = {key:[] for key in set(select_df.session_id.values)}\n",
    "for i in range(len(select_df)):\n",
    "    line = select_df.iloc[i]\n",
    "    session = line.feature\n",
    "    session_id = line.session_id\n",
    "    if ablation == 'TR':\n",
    "        LLM_pred = line.teacher_top_item\n",
    "        for j,item in enumerate(LLM_pred):\n",
    "            train_aug.append([session,item,session_id])\n",
    "\n",
    "    else:\n",
    "        LLM_pred = line.LLM_pred\n",
    "        for j,item in enumerate(LLM_pred):\n",
    "            if item >= data.n_item:\n",
    "                continue\n",
    "            if j<= 5:\n",
    "                train_aug.append([session,item,session_id])\n",
    "            if j<= 15:\n",
    "                train_aug.append([session,item,session_id])\n",
    "            if j<= 25:\n",
    "                train_aug.append([session,item,session_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22314286-d639-4f42-86a5-ff819bea27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23500\n"
     ]
    }
   ],
   "source": [
    "train_aug = pd.DataFrame(train_aug,columns=data.train.columns)\n",
    "print(len(train_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06dd5a03-7bcb-48f9-88ab-6741c84d9a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using time span 128\n",
      "total session: 23500\n"
     ]
    }
   ],
   "source": [
    "train_aug_data =  get_DataLoader(train_aug, 1024, seq_len=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb455172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "iter: 0, train loss: 0.5342, valid recall: 0.034077, valid ndcg: 0.009446\n",
      "time interval: 0.0081 min\n",
      "20\n",
      "iter: 20, train loss: 0.5456, valid recall: 0.034483, valid ndcg: 0.009543\n",
      "time interval: 0.0042 min\n",
      "40\n",
      "iter: 40, train loss: 0.5164, valid recall: 0.034888, valid ndcg: 0.009597\n",
      "time interval: 0.0042 min\n",
      "60\n",
      "iter: 60, train loss: 0.5083, valid recall: 0.036105, valid ndcg: 0.009878\n",
      "time interval: 0.0043 min\n",
      "80\n",
      "iter: 80, train loss: 0.5066, valid recall: 0.036511, valid ndcg: 0.009913\n",
      "time interval: 0.0043 min\n",
      "100\n",
      "iter: 100, train loss: 0.4801, valid recall: 0.038134, valid ndcg: 0.010405\n",
      "time interval: 0.0043 min\n",
      "120\n",
      "iter: 120, train loss: 0.4845, valid recall: 0.039351, valid ndcg: 0.010640\n",
      "time interval: 0.0050 min\n",
      "140\n",
      "iter: 140, train loss: 0.4761, valid recall: 0.040974, valid ndcg: 0.011027\n",
      "time interval: 0.0061 min\n",
      "160\n",
      "iter: 160, train loss: 0.4785, valid recall: 0.043002, valid ndcg: 0.011540\n",
      "time interval: 0.0047 min\n",
      "180\n",
      "iter: 180, train loss: 0.4503, valid recall: 0.043002, valid ndcg: 0.011554\n",
      "time interval: 0.0047 min\n",
      "200\n",
      "iter: 200, train loss: 0.4523, valid recall: 0.044625, valid ndcg: 0.011977\n",
      "time interval: 0.0042 min\n",
      "220\n",
      "iter: 220, train loss: 0.4299, valid recall: 0.045030, valid ndcg: 0.011998\n",
      "time interval: 0.0043 min\n",
      "240\n",
      "iter: 240, train loss: 0.4329, valid recall: 0.044625, valid ndcg: 0.011791\n",
      "time interval: 0.0043 min\n",
      "260\n",
      "iter: 260, train loss: 0.4117, valid recall: 0.045436, valid ndcg: 0.011938\n",
      "time interval: 0.0044 min\n",
      "280\n",
      "iter: 280, train loss: 0.4167, valid recall: 0.045436, valid ndcg: 0.011913\n",
      "time interval: 0.0042 min\n",
      "300\n",
      "iter: 300, train loss: 0.4106, valid recall: 0.046247, valid ndcg: 0.012175\n",
      "time interval: 0.0043 min\n",
      "320\n",
      "iter: 320, train loss: 0.3924, valid recall: 0.047465, valid ndcg: 0.012722\n",
      "time interval: 0.0043 min\n",
      "340\n",
      "iter: 340, train loss: 0.4036, valid recall: 0.047465, valid ndcg: 0.012879\n",
      "time interval: 0.0042 min\n",
      "360\n",
      "iter: 360, train loss: 0.3781, valid recall: 0.048276, valid ndcg: 0.012908\n",
      "time interval: 0.0052 min\n",
      "380\n",
      "iter: 380, train loss: 0.3851, valid recall: 0.047870, valid ndcg: 0.012806\n",
      "time interval: 0.0042 min\n",
      "400\n",
      "iter: 400, train loss: 0.3675, valid recall: 0.048276, valid ndcg: 0.012725\n",
      "time interval: 0.0046 min\n",
      "420\n",
      "iter: 420, train loss: 0.3743, valid recall: 0.048682, valid ndcg: 0.012870\n",
      "time interval: 0.0047 min\n",
      "440\n",
      "iter: 440, train loss: 0.3717, valid recall: 0.049087, valid ndcg: 0.012947\n",
      "time interval: 0.0043 min\n",
      "460\n",
      "iter: 460, train loss: 0.3771, valid recall: 0.049899, valid ndcg: 0.013098\n",
      "time interval: 0.0043 min\n",
      "480\n",
      "iter: 480, train loss: 0.3475, valid recall: 0.050710, valid ndcg: 0.013302\n",
      "time interval: 0.0064 min\n",
      "500\n",
      "iter: 500, train loss: 0.3506, valid recall: 0.050304, valid ndcg: 0.013187\n",
      "time interval: 0.0047 min\n",
      "520\n",
      "iter: 520, train loss: 0.3440, valid recall: 0.049899, valid ndcg: 0.013103\n",
      "time interval: 0.0042 min\n",
      "540\n",
      "iter: 540, train loss: 0.3422, valid recall: 0.050304, valid ndcg: 0.013160\n",
      "time interval: 0.0043 min\n",
      "560\n",
      "iter: 560, train loss: 0.3335, valid recall: 0.051116, valid ndcg: 0.013343\n",
      "time interval: 0.0043 min\n",
      "580\n",
      "iter: 580, train loss: 0.3424, valid recall: 0.051521, valid ndcg: 0.013446\n",
      "time interval: 0.0063 min\n",
      "600\n",
      "iter: 600, train loss: 0.3319, valid recall: 0.052333, valid ndcg: 0.013589\n",
      "time interval: 0.0042 min\n",
      "620\n",
      "iter: 620, train loss: 0.3278, valid recall: 0.053144, valid ndcg: 0.013693\n",
      "time interval: 0.0043 min\n",
      "640\n",
      "iter: 640, train loss: 0.3126, valid recall: 0.053955, valid ndcg: 0.013867\n",
      "time interval: 0.0043 min\n",
      "660\n",
      "iter: 660, train loss: 0.3165, valid recall: 0.053955, valid ndcg: 0.013869\n",
      "time interval: 0.0044 min\n",
      "680\n",
      "iter: 680, train loss: 0.3156, valid recall: 0.053955, valid ndcg: 0.013979\n",
      "time interval: 0.0042 min\n",
      "700\n",
      "iter: 700, train loss: 0.3088, valid recall: 0.054767, valid ndcg: 0.014151\n",
      "time interval: 0.0042 min\n",
      "720\n",
      "iter: 720, train loss: 0.3054, valid recall: 0.054767, valid ndcg: 0.014364\n",
      "time interval: 0.0046 min\n",
      "740\n",
      "iter: 740, train loss: 0.2945, valid recall: 0.054767, valid ndcg: 0.014278\n",
      "time interval: 0.0044 min\n",
      "760\n",
      "iter: 760, train loss: 0.3003, valid recall: 0.054361, valid ndcg: 0.014215\n",
      "time interval: 0.0045 min\n",
      "780\n",
      "iter: 780, train loss: 0.2921, valid recall: 0.055172, valid ndcg: 0.014582\n",
      "time interval: 0.0046 min\n",
      "800\n",
      "iter: 800, train loss: 0.2908, valid recall: 0.054767, valid ndcg: 0.014431\n",
      "time interval: 0.0050 min\n",
      "820\n",
      "iter: 820, train loss: 0.2765, valid recall: 0.055172, valid ndcg: 0.014565\n",
      "time interval: 0.0044 min\n",
      "840\n",
      "iter: 840, train loss: 0.2773, valid recall: 0.055172, valid ndcg: 0.014503\n",
      "time interval: 0.0042 min\n",
      "860\n",
      "iter: 860, train loss: 0.2815, valid recall: 0.054767, valid ndcg: 0.014411\n",
      "time interval: 0.0048 min\n",
      "880\n",
      "iter: 880, train loss: 0.2806, valid recall: 0.054361, valid ndcg: 0.014319\n",
      "time interval: 0.0043 min\n",
      "900\n",
      "iter: 900, train loss: 0.2883, valid recall: 0.054361, valid ndcg: 0.014319\n",
      "time interval: 0.0046 min\n",
      "920\n",
      "iter: 920, train loss: 0.2864, valid recall: 0.054361, valid ndcg: 0.014330\n",
      "time interval: 0.0043 min\n",
      "940\n",
      "iter: 940, train loss: 0.2706, valid recall: 0.054361, valid ndcg: 0.014310\n",
      "time interval: 0.0049 min\n",
      "960\n",
      "iter: 960, train loss: 0.2557, valid recall: 0.053955, valid ndcg: 0.014215\n",
      "time interval: 0.0042 min\n",
      "980\n",
      "iter: 980, train loss: 0.2672, valid recall: 0.052738, valid ndcg: 0.013962\n",
      "time interval: 0.0043 min\n",
      "1000\n",
      "iter: 1000, train loss: 0.2596, valid recall: 0.052738, valid ndcg: 0.013950\n",
      "time interval: 0.0050 min\n",
      "1020\n",
      "iter: 1020, train loss: 0.2606, valid recall: 0.052333, valid ndcg: 0.013906\n",
      "time interval: 0.0049 min\n",
      "1040\n",
      "iter: 1040, train loss: 0.2664, valid recall: 0.052333, valid ndcg: 0.013918\n",
      "time interval: 0.0045 min\n",
      "1060\n",
      "iter: 1060, train loss: 0.2576, valid recall: 0.053144, valid ndcg: 0.014188\n",
      "time interval: 0.0042 min\n",
      "1080\n",
      "iter: 1080, train loss: 0.2560, valid recall: 0.053550, valid ndcg: 0.014202\n",
      "time interval: 0.0043 min\n",
      "1100\n",
      "iter: 1100, train loss: 0.2493, valid recall: 0.052738, valid ndcg: 0.014039\n",
      "time interval: 0.0043 min\n",
      "1120\n",
      "iter: 1120, train loss: 0.2582, valid recall: 0.052738, valid ndcg: 0.014003\n",
      "time interval: 0.0043 min\n",
      "1140\n",
      "iter: 1140, train loss: 0.2429, valid recall: 0.052738, valid ndcg: 0.014032\n",
      "time interval: 0.0043 min\n",
      "1160\n",
      "iter: 1160, train loss: 0.2467, valid recall: 0.053955, valid ndcg: 0.014306\n",
      "time interval: 0.0043 min\n",
      "1180\n",
      "iter: 1180, train loss: 0.2413, valid recall: 0.053955, valid ndcg: 0.014300\n",
      "time interval: 0.0042 min\n",
      "1200\n",
      "iter: 1200, train loss: 0.2398, valid recall: 0.053550, valid ndcg: 0.014252\n",
      "early stopping!\n",
      "model loaded from ./best_model/ML+PFMC_S_distill+2024-08-14 17:32:35\n",
      "Valid recall: 0.055172, Valid ndcg: 0.014582\n",
      "Test result:\n",
      "test recall@5=0.055172\n",
      "test ndcg@5=0.014582\n",
      "test recall@10=0.019473\n",
      "test ndcg@10=0.008815\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.evaluation import evaluate\n",
    "from utils.log import load_model, save_model\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "def to_tensor(var, device):\n",
    "    var = torch.Tensor(var)\n",
    "    var = var.to(device)\n",
    "    return var.long()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)#, weight_decay=args.weight_decay)\n",
    "best_metric = 0\n",
    "for iter, (targets, items, mask,session_id) in enumerate(train_aug_data):\n",
    "    #训练\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    targets_cuda = to_tensor(targets,'cuda')\n",
    "    items_cuda = to_tensor(items,'cuda')\n",
    "    mask_cuda = to_tensor(mask,'cuda')\n",
    "    # negative_cuda = to_tensor(data.hard_negative_sample(session_id,negative_sampler,10),'cuda')\n",
    "    negative_cuda = to_tensor(data.uniform_negative_sample(targets_cuda,1),'cuda')\n",
    "    user_eb, scores = model(items_cuda,mask_cuda)\n",
    "    loss = model.loss(user_eb,targets_cuda,negative_cuda)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter % args.per_test  == 0:#\n",
    "        start_time = time.time()\n",
    "        print(iter)\n",
    "        model.eval()\n",
    "        metrics = evaluate(model, valid_data,50,args=args)\n",
    "        log_str = 'iter: %d, train loss: %.4f' % (iter, loss) # 打印loss\n",
    "        if metrics != {}:\n",
    "            log_str += ', ' + ', '.join(['valid ' + key + ': %.6f' % value for key, value in metrics.items()])\n",
    "        print(log_str)\n",
    "        log.write_str(log_str)\n",
    "        # 保存recall最佳的模型\n",
    "        if 'recall' in metrics:\n",
    "            recall = metrics['recall']\n",
    "            if recall > best_metric:\n",
    "                best_metric = recall\n",
    "                save_model(model, log.best_model_path)\n",
    "                trials = 0\n",
    "            else:\n",
    "                trials += 1\n",
    "                args.patience = 20 #if args.dataset =='rocket' else 3 \n",
    "                if trials > args.patience: # early stopping\n",
    "                    print(\"early stopping!\")\n",
    "                    break\n",
    "        # 每次test之后loss_sum置零\n",
    "        total_loss = 0.0\n",
    "        test_time = time.time()\n",
    "        print(\"time interval: %.4f min\" % ((test_time-start_time)/60.0))\n",
    "        sys.stdout.flush()\n",
    "    if iter >=  10000: # 超过最大迭代次数，退出训练\n",
    "        break\n",
    "\n",
    "load_model(model, log.best_model_path)\n",
    "model.eval()\n",
    "\n",
    "# 训练结束后用valid_data测试一次\n",
    "metrics = evaluate(model, valid_data,50,args=args)\n",
    "print(', '.join(['Valid ' + key + ': %.6f' % value for key, value in metrics.items()]))\n",
    "# 训练结束后用test_data测试一次\n",
    "print(\"Test result:\")\n",
    "for key, value in metrics.items():\n",
    "    output = 'test ' + key + '@5' + '=%.6f' % value\n",
    "    print(output)\n",
    "    log.write_str(output)\n",
    "metrics = evaluate(model, test_data,10,args=args)\n",
    "for key, value in metrics.items():\n",
    "    output = 'test ' + key + '@10' + '=%.6f' % value\n",
    "    print(output)\n",
    "    log.write_str(output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42702b9-e763-4945-8685-83c07cc81ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyp",
   "language": "python",
   "name": "dyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
